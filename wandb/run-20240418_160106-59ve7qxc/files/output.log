  0%|                                                                                                                 | 0/1000 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/ubuntu/verb-workspace/LLaMA-Factory/src/train_bash.py", line 14, in <module>
    main()
  File "/home/ubuntu/verb-workspace/LLaMA-Factory/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/ubuntu/verb-workspace/LLaMA-Factory/src/llmtuner/train/tuner.py", line 33, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/ubuntu/verb-workspace/LLaMA-Factory/src/llmtuner/train/sft/workflow.py", line 71, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/transformers/trainer.py", line 1859, in train
    return inner_training_loop(
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/transformers/trainer.py", line 2266, in _inner_training_loop
    self.optimizer.step()
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/accelerate/optimizer.py", line 161, in step
    self.optimizer.step(closure)
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/torch/optim/adamw.py", line 176, in step
    has_complex = self._init_group(
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/torch/optim/adamw.py", line 123, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 21.99 GiB of which 1.75 MiB is free. Process 62344 has 4.10 GiB memory in use. Process 295494 has 17.87 GiB memory in use. Of the allocated memory 17.36 GiB is allocated by PyTorch, and 237.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/ubuntu/verb-workspace/LLaMA-Factory/src/train_bash.py", line 14, in <module>
    main()
  File "/home/ubuntu/verb-workspace/LLaMA-Factory/src/train_bash.py", line 5, in main
    run_exp()
  File "/home/ubuntu/verb-workspace/LLaMA-Factory/src/llmtuner/train/tuner.py", line 33, in run_exp
    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
  File "/home/ubuntu/verb-workspace/LLaMA-Factory/src/llmtuner/train/sft/workflow.py", line 71, in run_sft
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/transformers/trainer.py", line 1859, in train
    return inner_training_loop(
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/transformers/trainer.py", line 2266, in _inner_training_loop
    self.optimizer.step()
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/accelerate/optimizer.py", line 161, in step
    self.optimizer.step(closure)
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/torch/optim/adamw.py", line 176, in step
    has_complex = self._init_group(
  File "/home/ubuntu/.pyenv/versions/3.10.14/lib/python3.10/site-packages/torch/optim/adamw.py", line 123, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacity of 21.99 GiB of which 1.75 MiB is free. Process 62344 has 4.10 GiB memory in use. Process 295494 has 17.87 GiB memory in use. Of the allocated memory 17.36 GiB is allocated by PyTorch, and 237.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)